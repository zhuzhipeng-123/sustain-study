{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4e196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#a1=np.zeros((3,2))\n",
    "#a2=np.ones((2,3))\n",
    "#np.random.rand(3,4)\n",
    "#axis 指的是被“压缩掉”的维度。如果为1则列压缩，为0行压缩\n",
    "#mask = np.triu(np.ones_like(A), k=1)  # 布尔或 float 掩码\n",
    "\n",
    "# 方法1：直接用 np.where\n",
    "#A_masked = np.where(mask == 1, -np.inf, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372cdf2",
   "metadata": {},
   "source": [
    "#单个attention实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08bba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#单个attention实现\n",
    "def init_qkv(X,l,d):\n",
    "    d=8\n",
    "    l=3\n",
    "    X=np.random.rand(l,d)#3代表几个token，8代表embedding层\n",
    "    W1=np.random.rand(d,d)\n",
    "    W2=np.random.rand(d,d)\n",
    "    W3=np.random.rand(d,d)\n",
    "    Q=X@W1\n",
    "    K=X@W2\n",
    "    V=X@W3\n",
    "    return Q,K,V\n",
    "##3个都是3*8的\n",
    "#自定义的sigmoid\n",
    "\n",
    "\n",
    "\n",
    "def softmax(X,a=-1):#1代表按照行求均值 ,axis=-1表示最后一个维度\n",
    "    x_shift=X-np.max(X,axis=a,keepdims=True)##同时减去一个最大值，保持整体不变\n",
    "    x_shift=np.exp(x_shift)\n",
    "    return x_shift/np.sum(x_shift,axis=a,keepdims=True)\n",
    "\n",
    "\n",
    "def attention_solo(Q,K,V):\n",
    "    if Q.ndim == 3:\n",
    "        KK=K.transpose(0,2,1)\n",
    "        score=softmax((Q@KK)/(np.sqrt(Q.shape[2])),-1)\n",
    "    else:\n",
    "        score=softmax((Q@K.T)/(np.sqrt(Q.shape[1])),-1)\n",
    "    attention=score@V\n",
    "    return attention\n",
    "\n",
    "\n",
    "def muti_att(n_heads,Q,K,V):\n",
    "    W1=np.random.rand(Q.shape[1],Q.shape[1])\n",
    "    \n",
    "    QQ=np.array(Q).reshape(Q.shape[0],n_heads,-1).transpose(1, 0, 2)                #需要注意attention是针对所有token进行操作\n",
    "    KK=np.array(K).reshape(K.shape[0],n_heads,-1).transpose(1, 0, 2)\n",
    "    VV=np.array(V).reshape(V.shape[0],n_heads,-1).transpose(1, 0, 2)\n",
    "    attention_plus=attention_solo(QQ,KK,VV)\n",
    "\n",
    "\n",
    "    attention_plus=attention_plus.transpose(1, 0, 2)\n",
    "    attention_plus=attention_plus.reshape(attention_plus.shape[0],-1)\n",
    "    final_attention=attention_plus@W1\n",
    "\n",
    "    return final_attention\n",
    "def positional_encoding(seq_len, d_model):\n",
    "\n",
    "    PE = np.zeros((seq_len, d_model))\n",
    "    position = np.arange(seq_len).reshape(-1, 1)  # (seq_len, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))  # (d_model//2,)  #numpy的广播机制是从右往左对齐的\n",
    "    \n",
    "    # 偶数列：sin\n",
    "    PE[:, 0::2] = np.sin(position * div_term)\n",
    "    # 奇数列：cos\n",
    "    PE[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return PE\n",
    "def lay_norm(X,a,b):\n",
    "    mean=X.mean(axis=-1,keepdims=True)\n",
    "    var = X.var(axis=-1, keepdims=True)  \n",
    "    \n",
    "    eps = 1e-5\n",
    "    X_norm = ((X - mean) / np.sqrt(var + eps))*a+b\n",
    "    return X_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c78983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ffn是一个二维度的线性变换\n",
    "\n",
    "\n",
    "def lay_norm(X,a=1.0,b=0.0):\n",
    "    mean=X.mean(axis=-1,keepdims=True)\n",
    "    var = X.var(axis=-1, keepdims=True)  \n",
    "    \n",
    "    eps = 1e-5\n",
    "    X_norm = ((X - mean) / np.sqrt(var + eps))*a+b\n",
    "    return X_norm\n",
    "\n",
    "def softmax(X,a=-1):#1代表按照行求均值 ,axis=-1表示最后一个维度\n",
    "    x_shift=X-np.max(X,axis=a,keepdims=True)##同时减去一个最大值，保持整体不变\n",
    "    x_shift=np.exp(x_shift)\n",
    "    return x_shift/np.sum(x_shift,axis=a,keepdims=True)\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self,X,sen_len,d_model,n_heads):\n",
    "        self.n_heads=n_heads\n",
    "        self.X=X\n",
    "        self.sen_len=sen_len\n",
    "        self.d_model=d_model\n",
    "\n",
    "        self.Wq = np.random.randn(d_model, d_model)\n",
    "        self.Wk = np.random.randn(d_model, d_model)\n",
    "        self.Wv = np.random.randn(d_model, d_model)\n",
    "        self.Wo = np.random.randn(d_model, d_model) \n",
    "\n",
    "        hidden_dim = d_model * 4\n",
    "        self.ffn_w1 = np.random.randn(d_model, hidden_dim)\n",
    "        self.ffn_w2 = np.random.randn(hidden_dim, d_model)\n",
    "\n",
    "    def init_qkv(self,X):\n",
    "        Q=X@self.Wq\n",
    "        K=X@self.Wk\n",
    "        V=X@self.Wv\n",
    "        return Q,K,V\n",
    "\n",
    "    def attention_solo(self,Q,K,V):\n",
    "        if Q.ndim == 3:\n",
    "            KK=K.transpose(0,2,1)\n",
    "            score=softmax((Q@KK)/(np.sqrt(Q.shape[2])),-1)\n",
    "        else:\n",
    "            score=softmax((Q@K.T)/(np.sqrt(Q.shape[1])),-1)\n",
    "        attention=score@V\n",
    "        return attention\n",
    "\n",
    "    def multi_head_attention(self,X):\n",
    "        Q, K, V = self.init_qkv(X) \n",
    "        QQ=np.array(Q).reshape(Q.shape[0],self.n_heads,-1).transpose(1, 0, 2)                #需要注意attention是针对所有token进行操作\n",
    "        KK=np.array(K).reshape(K.shape[0],self.n_heads,-1).transpose(1, 0, 2)\n",
    "        VV=np.array(V).reshape(V.shape[0],self.n_heads,-1).transpose(1, 0, 2)\n",
    "        attention_plus=self.attention_solo(QQ,KK,VV)\n",
    "\n",
    "\n",
    "        attention_plus=attention_plus.transpose(1, 0, 2)\n",
    "        attention_plus=attention_plus.reshape(attention_plus.shape[0],-1)\n",
    "        final_attention=attention_plus@self.Wo\n",
    "\n",
    "        return final_attention\n",
    "    \n",
    "\n",
    "    def positional_encoding(self):\n",
    "\n",
    "        PE = np.zeros((self.sen_len, self.d_model))\n",
    "        position = np.arange(self.sen_len).reshape(-1, 1)  # (seq_len, 1)\n",
    "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))  # (d_model//2,)  #numpy的广播机制是从右往左对齐的\n",
    "        \n",
    "        # 偶数列：sin\n",
    "        PE[:, 0::2] = np.sin(position * div_term)\n",
    "        # 奇数列：cos\n",
    "        PE[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        return PE\n",
    "    def ffn(self, X):\n",
    "        hidden = np.maximum(0, X @ self.ffn_w1)  # ReLU, (seq_len, hidden_dim)\n",
    "        output = hidden @ self.ffn_w2  \n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        # Step 1: Add positional encoding\n",
    "        X_pe = self.X + self.positional_encoding()  # (seq_len, d_model)\n",
    "\n",
    "        # Step 2: Multi-head attention + residual connection\n",
    "        attn_out = self.multi_head_attention(X_pe)  # (seq_len, d_model)\n",
    "        x1 = lay_norm(X_pe + attn_out)              # LayerNorm after residual\n",
    "\n",
    "        # Step 3: Feed-forward network + residual connection\n",
    "        ffn_out = self.ffn(x1)                      # (seq_len, d_model)\n",
    "        encoder_output = lay_norm(x1 + ffn_out)     # Final output\n",
    "        return encoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d72317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (5, 64)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(5, 64)\n",
    "encoder = Encoder(X, sen_len=5, d_model=64, n_heads=8)\n",
    "output = encoder.forward()\n",
    "print(\"Output shape:\", output.shape)  # (5, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac00644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3138bc6",
   "metadata": {},
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a60c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self,X,en_output,sen_len,de_model,nums_head):\n",
    "        self.X=X\n",
    "        self.en_output=en_output\n",
    "        self.sen_len=X.shape[0]\n",
    "        self.de_model=de_model\n",
    "        self.nums_head=nums_head\n",
    "        #ENCODER——decoder的参数\n",
    "        self.enco_q=np.random.rand(de_model,de_model)\n",
    "        self.enco_k=np.random.rand(de_model,de_model)\n",
    "        self.W_v_ed = np.random.randn(de_model, de_model)\n",
    "        self.W_o_ed = np.random.randn(de_model, de_model)\n",
    "\n",
    "\n",
    "        self.ffn1=np.random.rand(de_model,de_model*4)\n",
    "        self.ffn2=np.random.rand(de_model*4,de_model)\n",
    "\n",
    "        self.q=np.random.rand(de_model,de_model)\n",
    "        self.k=np.random.rand(de_model,de_model)\n",
    "        self.v=np.random.rand(de_model,de_model)\n",
    "        self.o=np.random.rand(de_model,de_model)\n",
    "        \n",
    "\n",
    "    def init_qkv(self,X):\n",
    "        Q=X@self.q\n",
    "        K=X@self.k\n",
    "        V=X@self.v\n",
    "        return Q,K,V\n",
    "    def mask(self,sen_len):\n",
    "        A = np.zeros((sen_len, sen_len))\n",
    "        mask = np.triu(np.ones_like(A), k=1)  # 布尔或 float 掩码\n",
    "        A_masked = np.where(mask == 1, -np.inf, A)\n",
    "        loss_mask=np.where(mask == 1,1,0)\n",
    "\n",
    "        return A_masked,loss_mask\n",
    "    \n",
    "    def split_heads(self,X):\n",
    "        QQ=np.array(X).reshape(X.shape[0],self.nums_head,-1)              #需要注意attention是针对所有token进行操作\n",
    "        return QQ\n",
    "        \n",
    "    def attention_mask_multi(self,Q,K,V,mask=None):\n",
    "        Q=self.split_heads(Q)\n",
    "        K=self.split_heads(K)\n",
    "        V=self.split_heads(V)\n",
    "        QQ=Q.transpose(1,0,2)\n",
    "        kk=K.transpose(1,2,0)\n",
    "        att=QQ@kk\n",
    "        if mask is not None:\n",
    "            att_mask=att+mask\n",
    "        else:\n",
    "            att_mask = att\n",
    "        score_mask=softmax((att_mask)/(np.sqrt(Q.shape[2])),-1)\n",
    "        vv=V.transpose(1,0,2)\n",
    "        att_v=score_mask@vv\n",
    "        att_final=att_v.transpose(1,0,2)\n",
    "        att_final=att_final.reshape(att_final.shape[0],-1)\n",
    "\n",
    "        return att_final\n",
    "    \n",
    "\n",
    "        #self.W_o_ed = np.random.randn(de_model, de_model)\n",
    "    def cross_attention(self,enc,dec,mask=None):\n",
    "        Q_dec=dec@self.enco_q\n",
    "\n",
    "        K_dec=enc@self.enco_k\n",
    "        V_dec=enc@self.W_v_ed\n",
    " \n",
    "        final=self.attention_mask_multi(Q_dec,K_dec,V_dec)\n",
    "        return final\n",
    "\n",
    "\n",
    "    def positional_encoding(self):\n",
    "\n",
    "        PE = np.zeros((self.sen_len, self.de_model))\n",
    "        position = np.arange(self.sen_len).reshape(-1, 1)  # (seq_len, 1)\n",
    "        div_term = np.exp(np.arange(0, self.de_model, 2) * -(np.log(10000.0) / self.de_model))  # (d_model//2,)  #numpy的广播机制是从右往左对齐的\n",
    "        \n",
    "        # 偶数列：sin\n",
    "        PE[:, 0::2] = np.sin(position * div_term)\n",
    "        # 奇数列：cos\n",
    "        PE[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "        return PE\n",
    "\n",
    "    def ffn(self,X):\n",
    "        relu=np.maximum(0, X @ self.ffn1) \n",
    "        outputt=relu@self.ffn2\n",
    "        return outputt\n",
    "    \n",
    "    def forward(self):\n",
    "        X_1=self.positional_encoding()+self.X\n",
    "        Q,K,V=self.init_qkv(X_1)\n",
    "        mask,loss=self.mask(self.sen_len)\n",
    "        atten1=(self.attention_mask_multi(Q,K,V,mask))@self.o\n",
    "        de1_out=lay_norm(atten1+X_1)\n",
    "\n",
    "\n",
    "        att2=self.cross_attention(self.en_output,de1_out,None)@self.W_o_ed\n",
    "        de2_out=lay_norm(att2+de1_out)\n",
    "\n",
    "        fin=lay_norm(self.ffn(de2_out)+de2_out)\n",
    "\n",
    "        return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f2c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "# 模拟输入\n",
    "X = np.random.randn(5, 64)          # decoder input embeddings\n",
    "\n",
    "encoder = Encoder(X, sen_len=5, d_model=64, n_heads=8)\n",
    "en_output = encoder.forward()\n",
    "\n",
    "decoder = Decoder(X, en_output, 5, 64, 8)\n",
    "output = decoder.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49378cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x, axis=-1):\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self, vob_size, seq_len, d_model, num_heads, en_layer, de_layer):\n",
    "        self.vob_size = vob_size\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.en_layer = en_layer\n",
    "        self.de_layer = de_layer\n",
    "        self.output_proj = np.random.randn(d_model, vob_size)  # 输出投影\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: (src_len, d_model) —— encoder input embeddings\n",
    "        tgt: (tgt_len, d_model) —— decoder input embeddings (e.g., shifted target)\n",
    "        \"\"\"\n",
    "        # ===== Encoder stack =====\n",
    "        enc_output = src\n",
    "        for _ in range(self.en_layer):\n",
    "            encoder = Encoder(enc_output, self.seq_len, self.d_model, self.num_heads)\n",
    "            enc_output = encoder.forward()  # 返回 (src_len, d_model)\n",
    "\n",
    "        # ===== Decoder stack =====\n",
    "        dec_output = tgt\n",
    "        for _ in range(self.de_layer):\n",
    "            decoder = Decoder(dec_output, enc_output, self.seq_len, self.d_model, self.num_heads)\n",
    "            dec_output = decoder.forward()  # 返回 (tgt_len, d_model)\n",
    "\n",
    "        logits = dec_output @ self.output_proj  # (tgt_len, vob_size)\n",
    "        output = softmax(logits, axis=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8d7d77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source input shape: (6, 32)\n",
      "Target input shape: (5, 32)\n",
      "Model output shape: (5, 1000)\n",
      "Output sum per token (should be ～1): [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子（可复现）\n",
    "np.random.seed(4)\n",
    "\n",
    "# 超参数\n",
    "vob_size = 1000      # 词表大小\n",
    "src_len = 6          # 源序列长度\n",
    "tgt_len = 5          # 目标序列长度\n",
    "d_model = 32         # 模型维度\n",
    "num_heads = 4\n",
    "en_layer = 2\n",
    "de_layer = 2\n",
    "\n",
    "# 模拟 embedding 输入（通常由 Embedding 层生成）\n",
    "src_embed = np.random.randn(src_len, d_model)  # (6, 32)\n",
    "tgt_embed = np.random.randn(tgt_len, d_model)  # (5, 32)\n",
    "\n",
    "# 创建 Transformer 模型\n",
    "model = Transformer(\n",
    "    vob_size=vob_size,\n",
    "    seq_len=max(src_len, tgt_len),  # 注意：这里简化处理，实际应分别处理 src/tgt len\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    en_layer=en_layer,\n",
    "    de_layer=de_layer\n",
    ")\n",
    "\n",
    "# 前向传播\n",
    "output = model.forward(src_embed, tgt_embed)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Source input shape:\", src_embed.shape)      # (6, 32)\n",
    "print(\"Target input shape:\", tgt_embed.shape)      # (5, 32)\n",
    "print(\"Model output shape:\", output.shape)         # (5, 1000)\n",
    "print(\"Output sum per token (should be ～1):\", np.round(output.sum(axis=-1), decimals=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3a4329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length) :\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        PE=torch.zeros(max_seq_length,d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        ev=position*div_term\n",
    "\n",
    "        PE[:,1::2]=torch.cos(ev)\n",
    "        PE[:,0::2]=torch.sin(ev)\n",
    "        self.register_buffer('PE', PE.unsqueeze(0))  # 添加 batch 维度 -> (1, max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        seq_len = X.size(1)\n",
    "        \n",
    "        return X + self.PE[:, :seq_len, :]  # 自动广播\n",
    "\n",
    "\n",
    "class ffn(nn.Module):\n",
    "    def __init__(self, d_model,fc) :\n",
    "        super(ffn,self).__init__()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.ffn=nn.Linear(d_model,fc)\n",
    "        self.ffn1=nn.Linear(fc,d_model)\n",
    "        self.norm=nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out1=self.ffn1(self.relu(self.ffn(x)))\n",
    "        return out1\n",
    "\n",
    "class multi_att(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super(multi_att, self).__init__()\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.d_k=d_model//num_heads\n",
    "\n",
    "        #qkv\n",
    "        self.q=nn.Linear(d_model,d_model)\n",
    "        self.k=nn.Linear(d_model,d_model)\n",
    "        self.v=nn.Linear(d_model,d_model)\n",
    "        self.o=nn.Linear(d_model,d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, L, D = x.shape\n",
    "        x = x.view(B, L, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)  # [B, H, L, d_k]\n",
    "\n",
    "\n",
    "    def att(self,Q,K,V,mask=None):\n",
    "        att=torch.matmul(Q,K.permute(0,1,3,2))/torch.sqrt(torch.tensor(self.d_k))\n",
    "        if mask is not None:\n",
    "            att = att.masked_fill(mask == 0, -1e9)#0的时候是-1e9\n",
    "        att_score=torch.matmul(torch.softmax(att,-1),V)\n",
    "       \n",
    "        B, H, L, d_k = att_score.shape\n",
    "        att_score = att_score.transpose(1, 2).contiguous()  # [B, L, H, d_k]\n",
    "        final_com = att_score.view(B, L, -1)\n",
    "        final_com=att_score.reshape(att_score.shape[0],att_score.shape[1],-1)\n",
    "        return final_com\n",
    "\n",
    "\n",
    "    def forward(self,qq,kk,vv,mask=None):\n",
    "        Q=self.split_heads(self.q(qq))\n",
    "        K=self.split_heads(self.k(kk))\n",
    "        V=self.split_heads(self.v(vv))\n",
    "\n",
    "        xxx=self.att(Q,K,V)\n",
    "        return self.o(xxx)\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,dropout,fc):\n",
    "        super(encoder, self).__init__()\n",
    "  \n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # 层归一化\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout\n",
    "\n",
    "        self.mt=multi_att(d_model,num_heads)\n",
    "        self.ffn=ffn(d_model,fc) \n",
    "        #\n",
    "    def forward(self,x,mask):\n",
    "        att=self.mt(x,x,x,mask)\n",
    "        out1=self.norm1(self.dropout(att)+x)\n",
    "        fin=self.norm2(out1+self.dropout(self.ffn(out1)))\n",
    "        return fin\n",
    "\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,d_model,dropout,num_heads,fc):\n",
    "        super(decoder, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)  # 层归一化\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout\n",
    "\n",
    "        self.mtt=multi_att(d_model,num_heads)\n",
    "        self.ffn=ffn(d_model,fc) \n",
    "    def forward(self,x,enc,mask_e,mask_d):\n",
    "        att=self.mtt(x,x,x,mask_e)\n",
    "        out1=self.norm1(self.dropout(att)+x)\n",
    "\n",
    "        att2=self.mtt(out1,enc,enc,mask_d)\n",
    "        att2=self.norm2(out1+self.dropout(att2))\n",
    "\n",
    "        fff=self.norm3(att2+self.dropout(self.ffn(att2)))\n",
    "\n",
    "        return  fff\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  # 编码器词嵌入\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  # 解码器词嵌入\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)  # 位置编码\n",
    "\n",
    "        # 编码器和解码器层\n",
    "        self.encoder_layers = nn.ModuleList([encoder(d_model,num_heads,dropout, d_ff) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([decoder(d_model,dropout,num_heads, d_ff) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)  # 最终的全连接层\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        # 源掩码：屏蔽填充符（假设填充符索引为0）\n",
    "        # 形状：(batch_size, 1, 1, seq_length)\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "   \n",
    "        # 目标掩码：屏蔽填充符和未来信息\n",
    "        # 形状：(batch_size, 1, seq_length, 1)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        # 生成上三角矩阵掩码，防止解码时看到未来信息\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask  # 合并填充掩码和未来信息掩码\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # 生成掩码\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "       \n",
    "        # 编码器部分\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "       \n",
    "        # 解码器部分\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "       \n",
    "        # 最终输出\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04c0b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.68264389038086\n",
      "Epoch: 2, Loss: 8.559000015258789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m optimizer.zero_grad()  \u001b[38;5;66;03m# 清空梯度，防止累积\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 输入目标序列时去掉最后一个词（用于预测下一个词）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m output = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 计算损失时，目标序列从第二个词开始（即预测下一个词）\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# output形状: (batch_size, seq_length-1, tgt_vocab_size)\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 目标形状: (batch_size, seq_length-1)\u001b[39;00m\n\u001b[32m     33\u001b[39m loss = criterion(\n\u001b[32m     34\u001b[39m     output.contiguous().view(-\u001b[32m1\u001b[39m, tgt_vocab_size), \n\u001b[32m     35\u001b[39m     tgt_data[:, \u001b[32m1\u001b[39m:].contiguous().view(-\u001b[32m1\u001b[39m)\n\u001b[32m     36\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt)\u001b[39m\n\u001b[32m    165\u001b[39m dec_output = tgt_embedded\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_layers:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     dec_output = \u001b[43mdec_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# 最终输出\u001b[39;00m\n\u001b[32m    170\u001b[39m output = \u001b[38;5;28mself\u001b[39m.fc(dec_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mdecoder.forward\u001b[39m\u001b[34m(self, x, enc, mask_e, mask_d)\u001b[39m\n\u001b[32m    116\u001b[39m att2=\u001b[38;5;28mself\u001b[39m.mtt(out1,enc,enc,mask_d)\n\u001b[32m    117\u001b[39m att2=\u001b[38;5;28mself\u001b[39m.norm2(out1+\u001b[38;5;28mself\u001b[39m.dropout(att2))\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m fff=\u001b[38;5;28mself\u001b[39m.norm3(att2+\u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43matt2\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m  fff\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mffn.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     out1=\u001b[38;5;28mself\u001b[39m.ffn1(\u001b[38;5;28mself\u001b[39m.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 超参数\n",
    "src_vocab_size = 5000  # 源词汇表大小\n",
    "tgt_vocab_size = 5000  # 目标词汇表大小\n",
    "d_model = 512  # 模型维度\n",
    "num_heads = 8  # 注意力头数量\n",
    "num_layers = 6  # 编码器和解码器层数\n",
    "d_ff = 2048  # 前馈网络内层维度\n",
    "max_seq_length = 100  # 最大序列长度\n",
    "dropout = 0.1  # Dropout 概率\n",
    "\n",
    "# 初始化模型\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# 生成随机数据\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # 源序列\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # 目标序列\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略填充部分的损失\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 训练循环\n",
    "transformer.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()  # 清空梯度，防止累积\n",
    "    \n",
    "    # 输入目标序列时去掉最后一个词（用于预测下一个词）\n",
    "    output = transformer(src_data, tgt_data[:, :-1])  \n",
    "    \n",
    "    # 计算损失时，目标序列从第二个词开始（即预测下一个词）\n",
    "    # output形状: (batch_size, seq_length-1, tgt_vocab_size)\n",
    "    # 目标形状: (batch_size, seq_length-1)\n",
    "    loss = criterion(\n",
    "        output.contiguous().view(-1, tgt_vocab_size), \n",
    "        tgt_data[:, 1:].contiguous().view(-1)\n",
    "    )\n",
    "    \n",
    "    loss.backward()        # 反向传播\n",
    "    optimizer.step()       # 更新参数\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bf9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(self, src, tgt):\n",
    "    # 源掩码：屏蔽填充符（假设填充符索引为0）\n",
    "    # 形状：(batch_size, 1, 1, seq_length)\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # 目标掩码：屏蔽填充符和未来信息\n",
    "    # 形状：(batch_size, 1, seq_length, 1)\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "    seq_length = tgt.size(1)\n",
    "    # 生成上三角矩阵掩码，防止解码时看到未来信息\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask  # 合并填充掩码和未来信息掩码\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 定义最大序列长度\n",
    "max_seq_length = 5\n",
    "\n",
    "# 创建一个批次大小为 2 的简单源序列和目标序列\n",
    "src_data = torch.tensor([\n",
    "    [2, 3, 4, 0, 0],  # 第一个样本，后两个是填充符\n",
    "    [2, 3, 4, 5, 6]   # 第二个样本，没有填充符\n",
    "])\n",
    "\n",
    "tgt_data = torch.tensor([\n",
    "    [1, 2, 3, 0, 0],  # 第一个样本，后两个是填充符\n",
    "    [1, 2, 3, 4, 5]   # 第二个样本，没有填充符\n",
    "])\n",
    "def generate_mask(src, tgt):\n",
    "    # 源掩码：屏蔽填充符（假设填充符索引为0）\n",
    "    #在计算注意力阶段，由于attention的维度是[B, H, src_len, src_len]，因此我们也需要一个这样的mask\n",
    "    #所以需要把encoder的拓展到[B, 1, 1, src_len]，这样利用广播机制，可以把剩下两个位置也加上\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # 目标掩码：屏蔽填充符和未来信息\n",
    "    #padding 必须屏蔽“列”（key） —— 因为 padding token 不该作为 key 被其他词注意！,因此广播机制应该是对列广播\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)#这里之所以这样，是为了方便和后续的nopeak_mask合并，利用广播机制\n",
    "\n",
    "\n",
    "    seq_length = tgt.size(1)\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask  # 合并填充掩码和未来信息掩码\n",
    "    \n",
    "  \n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "src_mask, tgt_mask = generate_mask(src_data, tgt_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20d107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
